{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "[(500, 'intercourse'), (501, 'lament'), (502, 'occupied'), (503, 'common'), (504, 'saturday'), (505, 'guarded'), (506, 'father'), (507, 'security'), (508, 'necessary'), (509, 'started')]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "filename = 'data/queries_lexicon.pkl'\n",
    "\n",
    "\n",
    "lexicon = pickle.load( open(filename, \"rb\"))\n",
    "\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = { idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    lexicon_lookup[0] = \"\" #map 0 padding to empty string\n",
    "    lexicon_lookup[793] = \"\"\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(list(lexicon_lookup.items())[500:510])\n",
    "    return lexicon_lookup\n",
    "\n",
    "lexicon_lookup = get_lexicon_lookup(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1374, 1451, 3189, 1275, 231, 2412, 2375, 506]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [['then', 'i', 'moved','with', 'fear', 'as', 'my', 'father']]\n",
    "\n",
    "def tokens_to_ids(all_tokens, lexicon):\n",
    "    ids = [[ lexicon[token] if token in lexicon else lexicon['<UNK>'] \\\n",
    "           for token in token_line] \\\n",
    "           for token_line in all_tokens]\n",
    "    return ids\n",
    "\n",
    "tester = tokens_to_ids(test_input, lexicon)\n",
    "tester\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_lookup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=None):\n",
    "\n",
    "    input_layer = Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "    \n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes,\n",
    "                               output_dim=n_embedding_nodes,\n",
    "                               mask_zero=True, name='embedding_layer')(input_layer)\n",
    "    \n",
    "    gru_layer1 = GRU(n_hidden_nodes,\n",
    "                    return_sequences=True,\n",
    "                    stateful=stateful,\n",
    "                    name='hidden_layer1')(embedding_layer)\n",
    "    \n",
    "    gru_layer2 = GRU(n_hidden_nodes,\n",
    "                    return_sequences=True,\n",
    "                    stateful=stateful,\n",
    "                    name='hidden_layer2')(gru_layer1)\n",
    "    \n",
    "    output_layer = TimeDistributed(Dense(n_input_nodes, activation=\"softmax\"),\n",
    "                                  name='output_layer')(gru_layer2)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor_model = create_model(seq_input_len=1,\n",
    "                              n_input_nodes=len(lexicon) + 1,\n",
    "                              n_embedding_nodes=300,\n",
    "                              n_hidden_nodes = 500,\n",
    "                              stateful=True,\n",
    "                              batch_size=1)\n",
    "\n",
    "predictor_model.load_weights('corpse_weights5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'creature', 'said', 'i', 'am', 'afraid', 'i', 'do', 'not', 'know']]\n",
      "\n",
      "-----------------\n",
      "how to say , the well of the side of what it could not be well , had great conduct before with an way so produced into all one exertion at netherfield , and bringing away the rest for him . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "\n",
    "def generate_ending(idx_seq):\n",
    "    end_of_sent_tokens = [\".\", \"!\",\"/\",\";\",\"?\",\":\"]\n",
    "    generated_ending = []\n",
    "    \n",
    "    if len(idx_seq) == 0:\n",
    "        return [3]\n",
    "    \n",
    "    for word in idx_seq:\n",
    "        p_next_word = predictor_model.predict(numpy.array(word)[None, None])[0,0]\n",
    "        \n",
    "    while not generated_ending or lexicon_lookup[next_word] not in end_of_sent_tokens:\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "\n",
    "        if next_word != 1:\n",
    "            generated_ending.append(next_word)\n",
    "            p_next_word = predictor_model.predict(numpy.array(next_word)[None, None])[0,0]\n",
    "        \n",
    "    predictor_model.reset_states()\n",
    "    return generated_ending\n",
    "\n",
    "\n",
    "test_input = [['the','creature','said','i','am','afraid','i','do','not','know']]\n",
    "\n",
    "tester = tokens_to_ids(test_input, lexicon)\n",
    "\n",
    "generated_ending = generate_ending(tester[0])\n",
    "if not generated_ending[0] == 2:\n",
    "    generated_ending = \" \".join([lexicon_lookup[word] if word in lexicon_lookup else \"\" \\\n",
    "                                 for word in generated_ending])\n",
    "    print(test_input)\n",
    "    print(\"\\n-----------------\")\n",
    "    print(generated_ending,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "encoder = spacy.load('en')\n",
    "#current_app.encoder = spacy.load('en')\n",
    "\n",
    "def text_to_tokens(lines):\n",
    "    tokens = [ [word.lower_ for word in encoder(line)] for line in lines]\n",
    "    #tokens = [ [word.lower_ for word in current_app.encoder(line)] for line in lines]\n",
    "    return tokens\n",
    "\n",
    "def lineSubmit(test_line):\n",
    "    input_line = tokens_to_ids( text_to_tokens(test_line), lexicon)\n",
    "    #input_line = tokens_to_ids( text_to_tokens(test_line), current_app.lexicon)\n",
    "\n",
    "    generated_ending = generate_ending(input_line[0])\n",
    "    if not generated_ending[0] == 2:\n",
    "        #generated_ending = \" \".join([current_app.lexicon_lookup[word] if word in current_app.lexicon_lookup else \"\" for word in generated_ending])\n",
    "        generated_ending = \" \".join([lexicon_lookup[word] if word in lexicon_lookup else \"\" for word in generated_ending])\n",
    "    \n",
    "    return generated_ending\n",
    "    #socketio.emit('line_append', { 'new_line':generated_ending}, namespace='/eq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was scarcely hid when a young girl came running towards the spot where i was concealed , laughing , as if she ran from someone in sport .\n"
     ]
    }
   ],
   "source": [
    "print(lineSubmit(\"I looked out the window and saw my sister. I said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>The bride and her mother could neither of them talk fast enough; and Wickham, who happened to sit near Elizabeth, began inquiring after his acquaintance in that neighbourhood, with a good humoured ease which she felt very unable to equal in her replies.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              line\n",
       "200  The bride and her mother could neither of them talk fast enough; and Wickham, who happened to sit near Elizabeth, began inquiring after his acquaintance in that neighbourhood, with a good humoured ease which she felt very unable to equal in her replies."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 300)\n",
    "\n",
    "train_lines = pandas.read_csv('data/queries.csv', encoding='utf-8')[200:240]\n",
    "train_lines[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total frankenstein sentences:  3339\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('data/frankenstein.txt') as f:\n",
    "    text = f.read()    \n",
    "    frank_sentences = re.split(r' *[\\.\\!][\\'\"\\)\\]]* *', text)\n",
    "    \n",
    "    for i in range(len(frank_sentences)):\n",
    "        frank_sentences[i] = frank_sentences[i].replace(\"\\n\", \" \") + \".\"\n",
    "        \n",
    "print(\"total frankenstein sentences: \", len(frank_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frank_sample = frank_sentences[2700:3000]\n",
    "len(frank_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But to a Genevan magistrate, whose mind was occupied by far other ideas than those of devotion and heroism, this elevation of mind had much the appearance of madness.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "start = random.randint(0,300)\n",
    "print(frank_sample[start])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "with open('data/frank_starters.txt') as f:\n",
    "    text = f.read()\n",
    "    frank_sentences = re.split(r' *[\\.\\!][\\'\"\\)\\]]* *', text)\n",
    "    \n",
    "    for i in range(len(frank_sentences)):\n",
    "        frank_sentences[i] = frank_sentences[i].replace(\"\\n\", \" \") + \".\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frank_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everybody believed that poor girl to be guilty; and if she could have committed the crime for which she suffered, assuredly she would have been the most depraved of human creatures.\n",
      "have ,  65\n",
      "first:  Everybody believed that poor girl to be guilty; and if she could \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "wordCount = 0\n",
    "\n",
    "while wordCount < 12:\n",
    "    starter = random.randint(0, len(frank_sentences))\n",
    "    rawfirst = frank_sentences[starter]\n",
    "    splitted = rawfirst.split()\n",
    "    wordCount = len(splitted)\n",
    "\n",
    "print(rawfirst)\n",
    "\n",
    "\n",
    "stopWord = splitted[12]\n",
    "\n",
    "stopIndex = rawfirst.find(stopWord)\n",
    "\n",
    "print(stopWord, \", \", stopIndex)                      \n",
    "                          \n",
    "if stopIndex > 3:\n",
    "    first = rawfirst[:stopIndex]\n",
    "    print(\"first: \", first)\n",
    "else:\n",
    "    print(\"rawfirst :\", rawfirst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everybody believed that poor girl to be guilty; and if she could \n",
      "is not to be when they know how    collins is many enough to be without turn hi   my plan is more enough to find there to say miss de bourgh , in a manner which have the more man at pemberley .\n"
     ]
    }
   ],
   "source": [
    "print(first)\n",
    "print(lineSubmit(first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frank_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
